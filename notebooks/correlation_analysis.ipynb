{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b997c9ec",
   "metadata": {},
   "source": [
    "# Task 3: Correlation Analysis Between News Sentiment and Stock Movements\n",
    "# Comprehensive Integration of Financial News and Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a946c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CORRELATION ANALYSIS: NEWS SENTIMENT & STOCK MOVEMENTS ===\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP and Sentiment Analysis\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== CORRELATION ANALYSIS: NEWS SENTIMENT & STOCK MOVEMENTS ===\")\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393b8c0",
   "metadata": {},
   "source": [
    "2 :Enhanced Sentiment Analysis: Enhanced Sentiment Analysis for Financial Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e2979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Sentiment Analyzer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class FinancialSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive sentiment analyzer tailored for financial news\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        # Financial lexicon enhancements\n",
    "        self.positive_financial_terms = {\n",
    "            'bullish', 'surge', 'rally', 'gain', 'profit', 'growth', 'beat', 'outperform',\n",
    "            'upgrade', 'buy', 'outperform', 'strong', 'positive', 'optimistic', 'recovery'\n",
    "        }\n",
    "        self.negative_financial_terms = {\n",
    "            'bearish', 'plunge', 'drop', 'loss', 'decline', 'miss', 'underperform',\n",
    "            'downgrade', 'sell', 'weak', 'negative', 'pessimistic', 'crash', 'slump'\n",
    "        }\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Perform comprehensive sentiment analysis using multiple methods\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return 0.0\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Method 1: TextBlob sentiment\n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            textblob_score = blob.sentiment.polarity\n",
    "        except:\n",
    "            textblob_score = 0.0\n",
    "        \n",
    "        # Method 2: VADER sentiment (specifically trained for social media/text)\n",
    "        vader_scores = self.sia.polarity_scores(text)\n",
    "        vader_score = vader_scores['compound']\n",
    "        \n",
    "        # Method 3: Financial term boosting\n",
    "        financial_boost = 0.0\n",
    "        positive_count = sum(1 for term in self.positive_financial_terms if term in text)\n",
    "        negative_count = sum(1 for term in self.negative_financial_terms if term in text)\n",
    "        \n",
    "        if positive_count > negative_count:\n",
    "            financial_boost = 0.1\n",
    "        elif negative_count > positive_count:\n",
    "            financial_boost = -0.1\n",
    "        \n",
    "        # Combined score (weighted average)\n",
    "        combined_score = (textblob_score * 0.4 + vader_score * 0.5 + financial_boost * 0.1)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        return max(-1.0, min(1.0, combined_score))\n",
    "    \n",
    "    def get_sentiment_label(self, score):\n",
    "        \"\"\"Convert sentiment score to categorical label\"\"\"\n",
    "        if score > 0.1:\n",
    "            return 'positive'\n",
    "        elif score < -0.1:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sentiment_analyzer = FinancialSentimentAnalyzer()\n",
    "\n",
    "print(\"Financial Sentiment Analyzer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626e1d5",
   "metadata": {},
   "source": [
    "3: Load and Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b8ae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING AND PREPARING DATASETS\n",
      "============================================================\n",
      "‚úì News data loaded: 1,407,328 articles\n",
      "‚úì AAPL data loaded: 3774 trading days\n",
      "‚úì AMZN data loaded: 3774 trading days\n",
      "‚úì GOOG data loaded: 3774 trading days\n",
      "‚úì META data loaded: 2923 trading days\n",
      "‚úì MSFT data loaded: 3774 trading days\n",
      "‚úì NVDA data loaded: 3774 trading days\n",
      "\n",
      "üìä Dataset Summary:\n",
      "   ‚Ä¢ News Articles: 1,407,328\n",
      "   ‚Ä¢ Stocks Loaded: 6\n",
      "   ‚Ä¢ News Date Range: 2009-02-14 00:00:00 to 2020-06-11 17:12:35-04:00\n",
      "\n",
      "Sample News Headlines:\n",
      "   1. Stocks That Hit 52-Week Highs On Friday\n",
      "   2. Stocks That Hit 52-Week Highs On Wednesday\n",
      "   3. 71 Biggest Movers From Friday\n",
      "   4. 46 Stocks Moving In Friday's Mid-Day Session\n",
      "   5. B of A Securities Maintains Neutral on Agilent Technologies, Raises Price Target to $88\n"
     ]
    }
   ],
   "source": [
    "# Load and Prepare Datasets for Correlation Analysis\n",
    "\n",
    "def load_and_prepare_datasets():\n",
    "    \"\"\"\n",
    "    Load both news and stock datasets and prepare for correlation analysis\n",
    "    \"\"\"\n",
    "    # Load news data (from Task 1)\n",
    "    try:\n",
    "        news_df = pd.read_csv('../data/raw_analyst_ratings.csv')\n",
    "        unnamed_cols = news_df.columns[news_df.columns.str.contains('Unnamed', case=False)]\n",
    "        news_df = news_df.drop(columns=unnamed_cols, axis=1)\n",
    "        print(f\"‚úì News data loaded: {len(news_df):,} articles\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚úó News data file not found. Please ensure 'data/financial_news.csv' exists.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load stock data (from Task 2)\n",
    "    stock_symbols = ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
    "    stock_data = {}\n",
    "    \n",
    "    for symbol in stock_symbols:\n",
    "        try:\n",
    "            stock_df = pd.read_csv(f'../data/yfinancedata/{symbol}.csv', index_col=0, parse_dates=True)\n",
    "            stock_data[symbol] = stock_df\n",
    "            print(f\"‚úì {symbol} data loaded: {len(stock_df)} trading days\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚úó {symbol} data file not found\")\n",
    "            continue\n",
    "    \n",
    "    return news_df, stock_data\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING AND PREPARING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "news_df, stock_data = load_and_prepare_datasets()\n",
    "\n",
    "if news_df is not None and stock_data:\n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    print(f\"   ‚Ä¢ News Articles: {len(news_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Stocks Loaded: {len(stock_data)}\")\n",
    "    print(f\"   ‚Ä¢ News Date Range: {news_df['date'].min()} to {news_df['date'].max()}\")\n",
    "    \n",
    "    # Display sample of news data\n",
    "    print(f\"\\nSample News Headlines:\")\n",
    "    for i, headline in enumerate(news_df['headline'].head(5)):\n",
    "        print(f\"   {i+1}. {headline}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load datasets. Please check file paths and availability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f502b",
   "metadata": {},
   "source": [
    "4: Date Alignment and Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "008c72d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATE ALIGNMENT AND DATA INTEGRATION\n",
      "============================================================\n",
      "Processing news data dates...\n",
      "‚úì News data processed: 1,407,328 articles with valid dates\n",
      "Performing sentiment analysis on news headlines...\n",
      "‚úì Sentiment analysis completed\n",
      "\n",
      "Integrating data for AAPL...\n",
      "   ‚úì Found 441 articles specifically for AAPL\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 61 days of data\n",
      "   ‚úì News coverage: 415 articles\n",
      "   ‚úì Date range: 2020-03-09 00:00:00 to 2020-06-10 00:00:00\n",
      "\n",
      "Integrating data for AMZN...\n",
      "   ‚úì Found 278 articles specifically for AMZN\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 28 days of data\n",
      "   ‚úì News coverage: 265 articles\n",
      "   ‚úì Date range: 2020-04-27 00:00:00 to 2020-06-10 00:00:00\n",
      "\n",
      "Integrating data for GOOG...\n",
      "   ‚úì Found 1,199 articles specifically for GOOG\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 352 days of data\n",
      "   ‚úì News coverage: 1,168 articles\n",
      "   ‚úì Date range: 2018-11-13 00:00:00 to 2020-06-10 00:00:00\n",
      "\n",
      "Integrating data for META...\n",
      "   ‚ö† No direct news found for META, using all financial news...\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 2029 days of data\n",
      "   ‚úì News coverage: 1,111,659 articles\n",
      "   ‚úì Date range: 2012-05-18 00:00:00 to 2020-06-11 00:00:00\n",
      "\n",
      "Integrating data for MSFT...\n",
      "   ‚ö† No direct news found for MSFT, using all financial news...\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 2757 days of data\n",
      "   ‚úì News coverage: 1,379,682 articles\n",
      "   ‚úì Date range: 2009-04-27 00:00:00 to 2020-06-11 00:00:00\n",
      "\n",
      "Integrating data for NVDA...\n",
      "   ‚úì Found 3,146 articles specifically for NVDA\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 1125 days of data\n",
      "   ‚úì News coverage: 3,070 articles\n",
      "   ‚úì Date range: 2011-03-03 00:00:00 to 2020-06-10 00:00:00\n",
      "\n",
      "‚úÖ SUCCESSFULLY INTEGRATED DATA FOR 6 STOCKS\n",
      "\n",
      "üìä AAPL Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 61\n",
      "   ‚Ä¢ Date Range: 2020-03-09 00:00:00 to 2020-06-10 00:00:00\n",
      "   ‚Ä¢ Total Articles: 415\n",
      "   ‚Ä¢ Average Articles/Day: 6.8\n",
      "   ‚Ä¢ Average Sentiment: 0.044\n",
      "   ‚Ä¢ Average Daily Return: 0.347%\n",
      "\n",
      "üìä AMZN Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 28\n",
      "   ‚Ä¢ Date Range: 2020-04-27 00:00:00 to 2020-06-10 00:00:00\n",
      "   ‚Ä¢ Total Articles: 265\n",
      "   ‚Ä¢ Average Articles/Day: 9.5\n",
      "   ‚Ä¢ Average Sentiment: 0.067\n",
      "   ‚Ä¢ Average Daily Return: 0.285%\n",
      "\n",
      "üìä GOOG Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 352\n",
      "   ‚Ä¢ Date Range: 2018-11-13 00:00:00 to 2020-06-10 00:00:00\n",
      "   ‚Ä¢ Total Articles: 1,168\n",
      "   ‚Ä¢ Average Articles/Day: 3.3\n",
      "   ‚Ä¢ Average Sentiment: 0.055\n",
      "   ‚Ä¢ Average Daily Return: 0.145%\n",
      "\n",
      "üìä META Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 2,029\n",
      "   ‚Ä¢ Date Range: 2012-05-18 00:00:00 to 2020-06-11 00:00:00\n",
      "   ‚Ä¢ Total Articles: 1,111,659\n",
      "   ‚Ä¢ Average Articles/Day: 547.9\n",
      "   ‚Ä¢ Average Sentiment: 0.056\n",
      "   ‚Ä¢ Average Daily Return: 0.115%\n",
      "   ‚Ä¢ Data Quality: 0 missing sentiment, 1 missing returns\n",
      "\n",
      "üìä MSFT Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 2,757\n",
      "   ‚Ä¢ Date Range: 2009-04-27 00:00:00 to 2020-06-11 00:00:00\n",
      "   ‚Ä¢ Total Articles: 1,379,682\n",
      "   ‚Ä¢ Average Articles/Day: 500.4\n",
      "   ‚Ä¢ Average Sentiment: 0.057\n",
      "   ‚Ä¢ Average Daily Return: 0.095%\n",
      "\n",
      "üìä NVDA Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 1,125\n",
      "   ‚Ä¢ Date Range: 2011-03-03 00:00:00 to 2020-06-10 00:00:00\n",
      "   ‚Ä¢ Total Articles: 3,070\n",
      "   ‚Ä¢ Average Articles/Day: 2.7\n",
      "   ‚Ä¢ Average Sentiment: 0.078\n",
      "   ‚Ä¢ Average Daily Return: 0.287%\n",
      "\n",
      "==================================================\n",
      "SAMPLE INTEGRATED DATA FOR AAPL\n",
      "==================================================\n",
      "                     Close  daily_return  sentiment_mean  article_count\n",
      "date_normalized                                                        \n",
      "2020-03-09       64.373756     -7.909217         -0.2133              3\n",
      "2020-03-10       69.010056      7.202157         -0.0367              8\n",
      "2020-03-11       66.613319     -3.473025         -0.0030             14\n",
      "2020-03-12       60.034924     -9.875496         -0.0764              5\n",
      "2020-03-13       67.227592     11.980808         -0.0004             11\n",
      "2020-03-16       58.578964    -12.864700          0.0275             11\n",
      "2020-03-17       61.154701      4.397034          0.0116             12\n",
      "2020-03-18       59.657631     -2.448005         -0.0021              4\n",
      "2020-03-19       59.200531     -0.766205          0.1057              2\n",
      "2020-03-20       55.442158     -6.348547          0.2564              2\n"
     ]
    }
   ],
   "source": [
    "# Date Alignment and Data Integration\n",
    "\n",
    "def align_and_integrate_data(news_df, stock_data):\n",
    "    \"\"\"\n",
    "    Align news and stock data by dates and integrate for analysis\n",
    "    \"\"\"\n",
    "    # Convert news date to datetime and normalize\n",
    "    print(\"Processing news data dates...\")\n",
    "    news_df['date'] = pd.to_datetime(\n",
    "        news_df['date'],\n",
    "        format='mixed',   # allow mixed formats\n",
    "        utc=True,         # output in UTC\n",
    "        errors='coerce'   # convert problematic dates to NaT instead of raising error\n",
    "    )\n",
    "    \n",
    "    # Remove any rows with invalid dates\n",
    "    news_df = news_df.dropna(subset=['date'])\n",
    "    news_df['date_normalized'] = news_df['date'].dt.date\n",
    "    print(f\"‚úì News data processed: {len(news_df):,} articles with valid dates\")\n",
    "    \n",
    "    # Perform sentiment analysis on all headlines\n",
    "    print(\"Performing sentiment analysis on news headlines...\")\n",
    "    news_df['sentiment_score'] = news_df['headline'].apply(\n",
    "        lambda x: sentiment_analyzer.analyze_sentiment(x)\n",
    "    )\n",
    "    news_df['sentiment_label'] = news_df['sentiment_score'].apply(\n",
    "        lambda x: sentiment_analyzer.get_sentiment_label(x)\n",
    "    )\n",
    "    print(\"‚úì Sentiment analysis completed\")\n",
    "    \n",
    "    # Create integrated dataset for each stock\n",
    "    integrated_data = {}\n",
    "    \n",
    "    for symbol, stock_df in stock_data.items():\n",
    "        print(f\"\\nIntegrating data for {symbol}...\")\n",
    "        \n",
    "        # Ensure stock data has date index and create normalized date column\n",
    "        stock_df = stock_df.copy()\n",
    "        stock_df.index = pd.to_datetime(stock_df.index)\n",
    "        stock_df['date_normalized'] = stock_df.index.date\n",
    "        \n",
    "        # Calculate daily returns\n",
    "        stock_df['daily_return'] = stock_df['Close'].pct_change() * 100\n",
    "        stock_df['daily_return_abs'] = stock_df['daily_return'].abs()\n",
    "        \n",
    "        # Filter news for this specific stock\n",
    "        # First try exact symbol match\n",
    "        stock_news = news_df[news_df['stock'] == symbol].copy()\n",
    "        \n",
    "        if len(stock_news) == 0:\n",
    "            print(f\"   ‚ö† No direct news found for {symbol}, using all financial news...\")\n",
    "            # If no direct stock match, use all financial news\n",
    "            stock_news = news_df.copy()\n",
    "        else:\n",
    "            print(f\"   ‚úì Found {len(stock_news):,} articles specifically for {symbol}\")\n",
    "        \n",
    "        # Aggregate daily sentiment\n",
    "        if len(stock_news) > 0:\n",
    "            # Ensure date_normalized is consistent type for grouping\n",
    "            stock_news['date_normalized'] = pd.to_datetime(stock_news['date_normalized'])\n",
    "            \n",
    "            daily_sentiment = stock_news.groupby('date_normalized').agg({\n",
    "                'sentiment_score': ['mean', 'count', 'std'],\n",
    "                'headline': 'count'\n",
    "            }).round(4)\n",
    "            \n",
    "            # Flatten column names\n",
    "            daily_sentiment.columns = ['sentiment_mean', 'sentiment_count', 'sentiment_std', 'article_count']\n",
    "            daily_sentiment = daily_sentiment.reset_index()\n",
    "            \n",
    "            # Reset stock_df index for merging and ensure consistent date type\n",
    "            stock_df_reset = stock_df.reset_index()\n",
    "            stock_df_reset['date_normalized'] = pd.to_datetime(stock_df_reset['date_normalized'])\n",
    "            \n",
    "            print(f\"   ‚Ä¢ Stock data dates: {stock_df_reset['date_normalized'].dtype}\")\n",
    "            print(f\"   ‚Ä¢ Sentiment data dates: {daily_sentiment['date_normalized'].dtype}\")\n",
    "            \n",
    "            # Merge with stock data using pd.merge\n",
    "            merged_data = pd.merge(\n",
    "                stock_df_reset,\n",
    "                daily_sentiment,\n",
    "                on='date_normalized',\n",
    "                how='inner'  # Only keep dates that exist in both datasets\n",
    "            )\n",
    "            \n",
    "            if len(merged_data) == 0:\n",
    "                print(f\"   ‚ö† No overlapping dates found between news and stock data for {symbol}\")\n",
    "                print(f\"   ‚Ä¢ Stock date range: {stock_df_reset['date_normalized'].min()} to {stock_df_reset['date_normalized'].max()}\")\n",
    "                print(f\"   ‚Ä¢ News date range: {daily_sentiment['date_normalized'].min()} to {daily_sentiment['date_normalized'].max()}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate lagged correlations (news today vs returns tomorrow)\n",
    "            merged_data['returns_tomorrow'] = merged_data['daily_return'].shift(-1)\n",
    "            merged_data['returns_next_week'] = merged_data['daily_return'].shift(-5)\n",
    "            \n",
    "            # Set date as index for consistency\n",
    "            merged_data.set_index('date_normalized', inplace=True)\n",
    "            \n",
    "            integrated_data[symbol] = merged_data\n",
    "            print(f\"   ‚úì Integrated {len(merged_data)} days of data\")\n",
    "            print(f\"   ‚úì News coverage: {merged_data['article_count'].sum():,} articles\")\n",
    "            print(f\"   ‚úì Date range: {merged_data.index.min()} to {merged_data.index.max()}\")\n",
    "        else:\n",
    "            print(f\"   ‚úó No news data available for integration with {symbol}\")\n",
    "    \n",
    "    return integrated_data\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATE ALIGNMENT AND DATA INTEGRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "integrated_data = align_and_integrate_data(news_df, stock_data)\n",
    "\n",
    "if integrated_data:\n",
    "    print(f\"\\n‚úÖ SUCCESSFULLY INTEGRATED DATA FOR {len(integrated_data)} STOCKS\")\n",
    "    \n",
    "    # Display detailed summary for each stock\n",
    "    for symbol, data in integrated_data.items():\n",
    "        print(f\"\\nüìä {symbol} Integration Summary:\")\n",
    "        print(f\"   ‚Ä¢ Integrated Days: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Date Range: {data.index.min()} to {data.index.max()}\")\n",
    "        print(f\"   ‚Ä¢ Total Articles: {data['article_count'].sum():,}\")\n",
    "        print(f\"   ‚Ä¢ Average Articles/Day: {data['article_count'].mean():.1f}\")\n",
    "        print(f\"   ‚Ä¢ Average Sentiment: {data['sentiment_mean'].mean():.3f}\")\n",
    "        print(f\"   ‚Ä¢ Average Daily Return: {data['daily_return'].mean():.3f}%\")\n",
    "        \n",
    "        # Check data quality\n",
    "        missing_sentiment = data['sentiment_mean'].isna().sum()\n",
    "        missing_returns = data['daily_return'].isna().sum()\n",
    "        if missing_sentiment > 0 or missing_returns > 0:\n",
    "            print(f\"   ‚Ä¢ Data Quality: {missing_sentiment} missing sentiment, {missing_returns} missing returns\")\n",
    "    \n",
    "    # Display sample data from first stock\n",
    "    first_symbol = list(integrated_data.keys())[0]\n",
    "    sample_data = integrated_data[first_symbol]\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"SAMPLE INTEGRATED DATA FOR {first_symbol}\")\n",
    "    print(\"=\"*50)\n",
    "    print(sample_data[['Close', 'daily_return', 'sentiment_mean', 'article_count']].head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data integration successful. Checking data compatibility...\")\n",
    "    \n",
    "    # Debug information\n",
    "    if news_df is not None:\n",
    "        print(f\"\\nüì∞ News Data Info:\")\n",
    "        print(f\"   ‚Ä¢ Total articles: {len(news_df):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique stocks in news: {news_df['stock'].nunique()}\")\n",
    "        print(f\"   ‚Ä¢ Sample stocks: {news_df['stock'].value_counts().head(10).index.tolist()}\")\n",
    "        print(f\"   ‚Ä¢ News date range: {news_df['date_normalized'].min()} to {news_df['date_normalized'].max()}\")\n",
    "        print(f\"   ‚Ä¢ News date type: {news_df['date_normalized'].dtype}\")\n",
    "    \n",
    "    if stock_data:\n",
    "        print(f\"\\nüìà Stock Data Info:\")\n",
    "        for symbol, data in stock_data.items():\n",
    "            data_copy = data.copy()\n",
    "            data_copy.index = pd.to_datetime(data_copy.index)\n",
    "            data_copy['date_normalized'] = data_copy.index.date\n",
    "            print(f\"   ‚Ä¢ {symbol}: {len(data)} days, {data_copy['date_normalized'].min()} to {data_copy['date_normalized'].max()}\")\n",
    "            print(f\"   ‚Ä¢ {symbol} date type: {data_copy['date_normalized'].dtype}\")\n",
    "    \n",
    "    # Check for overlapping date ranges\n",
    "    if news_df is not None and stock_data:\n",
    "        print(f\"\\nüîç Checking for date range overlaps...\")\n",
    "        news_min_date = news_df['date_normalized'].min()\n",
    "        news_max_date = news_df['date_normalized'].max()\n",
    "        \n",
    "        for symbol, data in stock_data.items():\n",
    "            data_copy = data.copy()\n",
    "            data_copy.index = pd.to_datetime(data_copy.index)\n",
    "            stock_min_date = data_copy.index.min().date()\n",
    "            stock_max_date = data_copy.index.max().date()\n",
    "            \n",
    "            overlap_start = max(news_min_date, stock_min_date)\n",
    "            overlap_end = min(news_max_date, stock_max_date)\n",
    "            \n",
    "            if overlap_start <= overlap_end:\n",
    "                print(f\"   ‚Ä¢ {symbol}: OVERLAP FOUND - {overlap_start} to {overlap_end}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {symbol}: NO OVERLAP - News: {news_min_date}-{news_max_date}, Stock: {stock_min_date}-{stock_max_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78ff0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week1venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
