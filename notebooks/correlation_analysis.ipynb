{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b997c9ec",
   "metadata": {},
   "source": [
    "# Task 3: Correlation Analysis Between News Sentiment and Stock Movements\n",
    "# Comprehensive Integration of Financial News and Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a946c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CORRELATION ANALYSIS: NEWS SENTIMENT & STOCK MOVEMENTS ===\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP and Sentiment Analysis\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== CORRELATION ANALYSIS: NEWS SENTIMENT & STOCK MOVEMENTS ===\")\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393b8c0",
   "metadata": {},
   "source": [
    "2 :Enhanced Sentiment Analysis: Enhanced Sentiment Analysis for Financial Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e2979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Sentiment Analyzer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class FinancialSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive sentiment analyzer tailored for financial news\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        # Financial lexicon enhancements\n",
    "        self.positive_financial_terms = {\n",
    "            'bullish', 'surge', 'rally', 'gain', 'profit', 'growth', 'beat', 'outperform',\n",
    "            'upgrade', 'buy', 'outperform', 'strong', 'positive', 'optimistic', 'recovery'\n",
    "        }\n",
    "        self.negative_financial_terms = {\n",
    "            'bearish', 'plunge', 'drop', 'loss', 'decline', 'miss', 'underperform',\n",
    "            'downgrade', 'sell', 'weak', 'negative', 'pessimistic', 'crash', 'slump'\n",
    "        }\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Perform comprehensive sentiment analysis using multiple methods\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return 0.0\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Method 1: TextBlob sentiment\n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            textblob_score = blob.sentiment.polarity\n",
    "        except:\n",
    "            textblob_score = 0.0\n",
    "        \n",
    "        # Method 2: VADER sentiment (specifically trained for social media/text)\n",
    "        vader_scores = self.sia.polarity_scores(text)\n",
    "        vader_score = vader_scores['compound']\n",
    "        \n",
    "        # Method 3: Financial term boosting\n",
    "        financial_boost = 0.0\n",
    "        positive_count = sum(1 for term in self.positive_financial_terms if term in text)\n",
    "        negative_count = sum(1 for term in self.negative_financial_terms if term in text)\n",
    "        \n",
    "        if positive_count > negative_count:\n",
    "            financial_boost = 0.1\n",
    "        elif negative_count > positive_count:\n",
    "            financial_boost = -0.1\n",
    "        \n",
    "        # Combined score (weighted average)\n",
    "        combined_score = (textblob_score * 0.4 + vader_score * 0.5 + financial_boost * 0.1)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        return max(-1.0, min(1.0, combined_score))\n",
    "    \n",
    "    def get_sentiment_label(self, score):\n",
    "        \"\"\"Convert sentiment score to categorical label\"\"\"\n",
    "        if score > 0.1:\n",
    "            return 'positive'\n",
    "        elif score < -0.1:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sentiment_analyzer = FinancialSentimentAnalyzer()\n",
    "\n",
    "print(\"Financial Sentiment Analyzer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626e1d5",
   "metadata": {},
   "source": [
    "3: Load and Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b8ae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING AND PREPARING DATASETS\n",
      "============================================================\n",
      "‚úì News data loaded: 1,407,328 articles\n",
      "‚úì AAPL data loaded: 3774 trading days\n",
      "‚úì AMZN data loaded: 3774 trading days\n",
      "‚úì GOOG data loaded: 3774 trading days\n",
      "‚úì META data loaded: 2923 trading days\n",
      "‚úì MSFT data loaded: 3774 trading days\n",
      "‚úì NVDA data loaded: 3774 trading days\n",
      "\n",
      "üìä Dataset Summary:\n",
      "   ‚Ä¢ News Articles: 1,407,328\n",
      "   ‚Ä¢ Stocks Loaded: 6\n",
      "   ‚Ä¢ News Date Range: 2009-02-14 00:00:00 to 2020-06-11 17:12:35-04:00\n",
      "\n",
      "Sample News Headlines:\n",
      "   1. Stocks That Hit 52-Week Highs On Friday\n",
      "   2. Stocks That Hit 52-Week Highs On Wednesday\n",
      "   3. 71 Biggest Movers From Friday\n",
      "   4. 46 Stocks Moving In Friday's Mid-Day Session\n",
      "   5. B of A Securities Maintains Neutral on Agilent Technologies, Raises Price Target to $88\n"
     ]
    }
   ],
   "source": [
    "# Load and Prepare Datasets for Correlation Analysis\n",
    "\n",
    "def load_and_prepare_datasets():\n",
    "    \"\"\"\n",
    "    Load both news and stock datasets and prepare for correlation analysis\n",
    "    \"\"\"\n",
    "    # Load news data (from Task 1)\n",
    "    try:\n",
    "        news_df = pd.read_csv('../data/raw_analyst_ratings.csv')\n",
    "        unnamed_cols = news_df.columns[news_df.columns.str.contains('Unnamed', case=False)]\n",
    "        news_df = news_df.drop(columns=unnamed_cols, axis=1)\n",
    "        print(f\"‚úì News data loaded: {len(news_df):,} articles\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚úó News data file not found. Please ensure 'data/financial_news.csv' exists.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load stock data (from Task 2)\n",
    "    stock_symbols = ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
    "    stock_data = {}\n",
    "    \n",
    "    for symbol in stock_symbols:\n",
    "        try:\n",
    "            stock_df = pd.read_csv(f'../data/yfinancedata/{symbol}.csv', index_col=0, parse_dates=True)\n",
    "            stock_data[symbol] = stock_df\n",
    "            print(f\"‚úì {symbol} data loaded: {len(stock_df)} trading days\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚úó {symbol} data file not found\")\n",
    "            continue\n",
    "    \n",
    "    return news_df, stock_data\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING AND PREPARING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "news_df, stock_data = load_and_prepare_datasets()\n",
    "\n",
    "if news_df is not None and stock_data:\n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    print(f\"   ‚Ä¢ News Articles: {len(news_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Stocks Loaded: {len(stock_data)}\")\n",
    "    print(f\"   ‚Ä¢ News Date Range: {news_df['date'].min()} to {news_df['date'].max()}\")\n",
    "    \n",
    "    # Display sample of news data\n",
    "    print(f\"\\nSample News Headlines:\")\n",
    "    for i, headline in enumerate(news_df['headline'].head(5)):\n",
    "        print(f\"   {i+1}. {headline}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load datasets. Please check file paths and availability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f502b",
   "metadata": {},
   "source": [
    "4: Date Alignment and Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "008c72d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATE ALIGNMENT AND DATA INTEGRATION\n",
      "============================================================\n",
      "Processing news data dates...\n",
      "‚úì News data processed: 1,407,328 articles with valid dates\n",
      "Performing sentiment analysis on news headlines...\n",
      "‚úì Sentiment analysis completed\n",
      "\n",
      "Integrating data for AAPL...\n",
      "   ‚úì Found 441 articles specifically for AAPL\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 61 days of data\n",
      "   ‚úì News coverage: 415 articles\n",
      "   ‚úì Date range: 2020-03-09 00:00:00 to 2020-06-10 00:00:00\n",
      "\n",
      "Integrating data for AMZN...\n",
      "   ‚úì Found 278 articles specifically for AMZN\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 28 days of data\n",
      "   ‚úì News coverage: 265 articles\n",
      "   ‚úì Date range: 2020-04-27 00:00:00 to 2020-06-10 00:00:00\n",
      "\n",
      "Integrating data for GOOG...\n",
      "   ‚úì Found 1,199 articles specifically for GOOG\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 352 days of data\n",
      "   ‚úì News coverage: 1,168 articles\n",
      "   ‚úì Date range: 2018-11-13 00:00:00 to 2020-06-10 00:00:00\n",
      "\n",
      "Integrating data for META...\n",
      "   ‚ö† No direct news found for META, using all financial news...\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 2029 days of data\n",
      "   ‚úì News coverage: 1,111,659 articles\n",
      "   ‚úì Date range: 2012-05-18 00:00:00 to 2020-06-11 00:00:00\n",
      "\n",
      "Integrating data for MSFT...\n",
      "   ‚ö† No direct news found for MSFT, using all financial news...\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 2757 days of data\n",
      "   ‚úì News coverage: 1,379,682 articles\n",
      "   ‚úì Date range: 2009-04-27 00:00:00 to 2020-06-11 00:00:00\n",
      "\n",
      "Integrating data for NVDA...\n",
      "   ‚úì Found 3,146 articles specifically for NVDA\n",
      "   ‚Ä¢ Stock data dates: datetime64[ns]\n",
      "   ‚Ä¢ Sentiment data dates: datetime64[ns]\n",
      "   ‚úì Integrated 1125 days of data\n",
      "   ‚úì News coverage: 3,070 articles\n",
      "   ‚úì Date range: 2011-03-03 00:00:00 to 2020-06-10 00:00:00\n",
      "\n",
      "‚úÖ SUCCESSFULLY INTEGRATED DATA FOR 6 STOCKS\n",
      "\n",
      "üìä AAPL Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 61\n",
      "   ‚Ä¢ Date Range: 2020-03-09 00:00:00 to 2020-06-10 00:00:00\n",
      "   ‚Ä¢ Total Articles: 415\n",
      "   ‚Ä¢ Average Articles/Day: 6.8\n",
      "   ‚Ä¢ Average Sentiment: 0.044\n",
      "   ‚Ä¢ Average Daily Return: 0.347%\n",
      "\n",
      "üìä AMZN Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 28\n",
      "   ‚Ä¢ Date Range: 2020-04-27 00:00:00 to 2020-06-10 00:00:00\n",
      "   ‚Ä¢ Total Articles: 265\n",
      "   ‚Ä¢ Average Articles/Day: 9.5\n",
      "   ‚Ä¢ Average Sentiment: 0.067\n",
      "   ‚Ä¢ Average Daily Return: 0.285%\n",
      "\n",
      "üìä GOOG Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 352\n",
      "   ‚Ä¢ Date Range: 2018-11-13 00:00:00 to 2020-06-10 00:00:00\n",
      "   ‚Ä¢ Total Articles: 1,168\n",
      "   ‚Ä¢ Average Articles/Day: 3.3\n",
      "   ‚Ä¢ Average Sentiment: 0.055\n",
      "   ‚Ä¢ Average Daily Return: 0.145%\n",
      "\n",
      "üìä META Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 2,029\n",
      "   ‚Ä¢ Date Range: 2012-05-18 00:00:00 to 2020-06-11 00:00:00\n",
      "   ‚Ä¢ Total Articles: 1,111,659\n",
      "   ‚Ä¢ Average Articles/Day: 547.9\n",
      "   ‚Ä¢ Average Sentiment: 0.056\n",
      "   ‚Ä¢ Average Daily Return: 0.115%\n",
      "   ‚Ä¢ Data Quality: 0 missing sentiment, 1 missing returns\n",
      "\n",
      "üìä MSFT Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 2,757\n",
      "   ‚Ä¢ Date Range: 2009-04-27 00:00:00 to 2020-06-11 00:00:00\n",
      "   ‚Ä¢ Total Articles: 1,379,682\n",
      "   ‚Ä¢ Average Articles/Day: 500.4\n",
      "   ‚Ä¢ Average Sentiment: 0.057\n",
      "   ‚Ä¢ Average Daily Return: 0.095%\n",
      "\n",
      "üìä NVDA Integration Summary:\n",
      "   ‚Ä¢ Integrated Days: 1,125\n",
      "   ‚Ä¢ Date Range: 2011-03-03 00:00:00 to 2020-06-10 00:00:00\n",
      "   ‚Ä¢ Total Articles: 3,070\n",
      "   ‚Ä¢ Average Articles/Day: 2.7\n",
      "   ‚Ä¢ Average Sentiment: 0.078\n",
      "   ‚Ä¢ Average Daily Return: 0.287%\n",
      "\n",
      "==================================================\n",
      "SAMPLE INTEGRATED DATA FOR AAPL\n",
      "==================================================\n",
      "                     Close  daily_return  sentiment_mean  article_count\n",
      "date_normalized                                                        \n",
      "2020-03-09       64.373756     -7.909217         -0.2133              3\n",
      "2020-03-10       69.010056      7.202157         -0.0367              8\n",
      "2020-03-11       66.613319     -3.473025         -0.0030             14\n",
      "2020-03-12       60.034924     -9.875496         -0.0764              5\n",
      "2020-03-13       67.227592     11.980808         -0.0004             11\n",
      "2020-03-16       58.578964    -12.864700          0.0275             11\n",
      "2020-03-17       61.154701      4.397034          0.0116             12\n",
      "2020-03-18       59.657631     -2.448005         -0.0021              4\n",
      "2020-03-19       59.200531     -0.766205          0.1057              2\n",
      "2020-03-20       55.442158     -6.348547          0.2564              2\n"
     ]
    }
   ],
   "source": [
    "# Date Alignment and Data Integration\n",
    "\n",
    "def align_and_integrate_data(news_df, stock_data):\n",
    "    \"\"\"\n",
    "    Align news and stock data by dates and integrate for analysis\n",
    "    \"\"\"\n",
    "    # Convert news date to datetime and normalize\n",
    "    print(\"Processing news data dates...\")\n",
    "    news_df['date'] = pd.to_datetime(\n",
    "        news_df['date'],\n",
    "        format='mixed',   # allow mixed formats\n",
    "        utc=True,         # output in UTC\n",
    "        errors='coerce'   # convert problematic dates to NaT instead of raising error\n",
    "    )\n",
    "    \n",
    "    # Remove any rows with invalid dates\n",
    "    news_df = news_df.dropna(subset=['date'])\n",
    "    news_df['date_normalized'] = news_df['date'].dt.date\n",
    "    print(f\"‚úì News data processed: {len(news_df):,} articles with valid dates\")\n",
    "    \n",
    "    # Perform sentiment analysis on all headlines\n",
    "    print(\"Performing sentiment analysis on news headlines...\")\n",
    "    news_df['sentiment_score'] = news_df['headline'].apply(\n",
    "        lambda x: sentiment_analyzer.analyze_sentiment(x)\n",
    "    )\n",
    "    news_df['sentiment_label'] = news_df['sentiment_score'].apply(\n",
    "        lambda x: sentiment_analyzer.get_sentiment_label(x)\n",
    "    )\n",
    "    print(\"‚úì Sentiment analysis completed\")\n",
    "    \n",
    "    # Create integrated dataset for each stock\n",
    "    integrated_data = {}\n",
    "    \n",
    "    for symbol, stock_df in stock_data.items():\n",
    "        print(f\"\\nIntegrating data for {symbol}...\")\n",
    "        \n",
    "        # Ensure stock data has date index and create normalized date column\n",
    "        stock_df = stock_df.copy()\n",
    "        stock_df.index = pd.to_datetime(stock_df.index)\n",
    "        stock_df['date_normalized'] = stock_df.index.date\n",
    "        \n",
    "        # Calculate daily returns\n",
    "        stock_df['daily_return'] = stock_df['Close'].pct_change() * 100\n",
    "        stock_df['daily_return_abs'] = stock_df['daily_return'].abs()\n",
    "        \n",
    "        # Filter news for this specific stock\n",
    "        # First try exact symbol match\n",
    "        stock_news = news_df[news_df['stock'] == symbol].copy()\n",
    "        \n",
    "        if len(stock_news) == 0:\n",
    "            print(f\"   ‚ö† No direct news found for {symbol}, using all financial news...\")\n",
    "            # If no direct stock match, use all financial news\n",
    "            stock_news = news_df.copy()\n",
    "        else:\n",
    "            print(f\"   ‚úì Found {len(stock_news):,} articles specifically for {symbol}\")\n",
    "        \n",
    "        # Aggregate daily sentiment\n",
    "        if len(stock_news) > 0:\n",
    "            # Ensure date_normalized is consistent type for grouping\n",
    "            stock_news['date_normalized'] = pd.to_datetime(stock_news['date_normalized'])\n",
    "            \n",
    "            daily_sentiment = stock_news.groupby('date_normalized').agg({\n",
    "                'sentiment_score': ['mean', 'count', 'std'],\n",
    "                'headline': 'count'\n",
    "            }).round(4)\n",
    "            \n",
    "            # Flatten column names\n",
    "            daily_sentiment.columns = ['sentiment_mean', 'sentiment_count', 'sentiment_std', 'article_count']\n",
    "            daily_sentiment = daily_sentiment.reset_index()\n",
    "            \n",
    "            # Reset stock_df index for merging and ensure consistent date type\n",
    "            stock_df_reset = stock_df.reset_index()\n",
    "            stock_df_reset['date_normalized'] = pd.to_datetime(stock_df_reset['date_normalized'])\n",
    "            \n",
    "            print(f\"   ‚Ä¢ Stock data dates: {stock_df_reset['date_normalized'].dtype}\")\n",
    "            print(f\"   ‚Ä¢ Sentiment data dates: {daily_sentiment['date_normalized'].dtype}\")\n",
    "            \n",
    "            # Merge with stock data using pd.merge\n",
    "            merged_data = pd.merge(\n",
    "                stock_df_reset,\n",
    "                daily_sentiment,\n",
    "                on='date_normalized',\n",
    "                how='inner'  # Only keep dates that exist in both datasets\n",
    "            )\n",
    "            \n",
    "            if len(merged_data) == 0:\n",
    "                print(f\"   ‚ö† No overlapping dates found between news and stock data for {symbol}\")\n",
    "                print(f\"   ‚Ä¢ Stock date range: {stock_df_reset['date_normalized'].min()} to {stock_df_reset['date_normalized'].max()}\")\n",
    "                print(f\"   ‚Ä¢ News date range: {daily_sentiment['date_normalized'].min()} to {daily_sentiment['date_normalized'].max()}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate lagged correlations (news today vs returns tomorrow)\n",
    "            merged_data['returns_tomorrow'] = merged_data['daily_return'].shift(-1)\n",
    "            merged_data['returns_next_week'] = merged_data['daily_return'].shift(-5)\n",
    "            \n",
    "            # Set date as index for consistency\n",
    "            merged_data.set_index('date_normalized', inplace=True)\n",
    "            \n",
    "            integrated_data[symbol] = merged_data\n",
    "            print(f\"   ‚úì Integrated {len(merged_data)} days of data\")\n",
    "            print(f\"   ‚úì News coverage: {merged_data['article_count'].sum():,} articles\")\n",
    "            print(f\"   ‚úì Date range: {merged_data.index.min()} to {merged_data.index.max()}\")\n",
    "        else:\n",
    "            print(f\"   ‚úó No news data available for integration with {symbol}\")\n",
    "    \n",
    "    return integrated_data\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATE ALIGNMENT AND DATA INTEGRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "integrated_data = align_and_integrate_data(news_df, stock_data)\n",
    "\n",
    "if integrated_data:\n",
    "    print(f\"\\n‚úÖ SUCCESSFULLY INTEGRATED DATA FOR {len(integrated_data)} STOCKS\")\n",
    "    \n",
    "    # Display detailed summary for each stock\n",
    "    for symbol, data in integrated_data.items():\n",
    "        print(f\"\\nüìä {symbol} Integration Summary:\")\n",
    "        print(f\"   ‚Ä¢ Integrated Days: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Date Range: {data.index.min()} to {data.index.max()}\")\n",
    "        print(f\"   ‚Ä¢ Total Articles: {data['article_count'].sum():,}\")\n",
    "        print(f\"   ‚Ä¢ Average Articles/Day: {data['article_count'].mean():.1f}\")\n",
    "        print(f\"   ‚Ä¢ Average Sentiment: {data['sentiment_mean'].mean():.3f}\")\n",
    "        print(f\"   ‚Ä¢ Average Daily Return: {data['daily_return'].mean():.3f}%\")\n",
    "        \n",
    "        # Check data quality\n",
    "        missing_sentiment = data['sentiment_mean'].isna().sum()\n",
    "        missing_returns = data['daily_return'].isna().sum()\n",
    "        if missing_sentiment > 0 or missing_returns > 0:\n",
    "            print(f\"   ‚Ä¢ Data Quality: {missing_sentiment} missing sentiment, {missing_returns} missing returns\")\n",
    "    \n",
    "    # Display sample data from first stock\n",
    "    first_symbol = list(integrated_data.keys())[0]\n",
    "    sample_data = integrated_data[first_symbol]\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"SAMPLE INTEGRATED DATA FOR {first_symbol}\")\n",
    "    print(\"=\"*50)\n",
    "    print(sample_data[['Close', 'daily_return', 'sentiment_mean', 'article_count']].head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data integration successful. Checking data compatibility...\")\n",
    "    \n",
    "    # Debug information\n",
    "    if news_df is not None:\n",
    "        print(f\"\\nüì∞ News Data Info:\")\n",
    "        print(f\"   ‚Ä¢ Total articles: {len(news_df):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique stocks in news: {news_df['stock'].nunique()}\")\n",
    "        print(f\"   ‚Ä¢ Sample stocks: {news_df['stock'].value_counts().head(10).index.tolist()}\")\n",
    "        print(f\"   ‚Ä¢ News date range: {news_df['date_normalized'].min()} to {news_df['date_normalized'].max()}\")\n",
    "        print(f\"   ‚Ä¢ News date type: {news_df['date_normalized'].dtype}\")\n",
    "    \n",
    "    if stock_data:\n",
    "        print(f\"\\nüìà Stock Data Info:\")\n",
    "        for symbol, data in stock_data.items():\n",
    "            data_copy = data.copy()\n",
    "            data_copy.index = pd.to_datetime(data_copy.index)\n",
    "            data_copy['date_normalized'] = data_copy.index.date\n",
    "            print(f\"   ‚Ä¢ {symbol}: {len(data)} days, {data_copy['date_normalized'].min()} to {data_copy['date_normalized'].max()}\")\n",
    "            print(f\"   ‚Ä¢ {symbol} date type: {data_copy['date_normalized'].dtype}\")\n",
    "    \n",
    "    # Check for overlapping date ranges\n",
    "    if news_df is not None and stock_data:\n",
    "        print(f\"\\nüîç Checking for date range overlaps...\")\n",
    "        news_min_date = news_df['date_normalized'].min()\n",
    "        news_max_date = news_df['date_normalized'].max()\n",
    "        \n",
    "        for symbol, data in stock_data.items():\n",
    "            data_copy = data.copy()\n",
    "            data_copy.index = pd.to_datetime(data_copy.index)\n",
    "            stock_min_date = data_copy.index.min().date()\n",
    "            stock_max_date = data_copy.index.max().date()\n",
    "            \n",
    "            overlap_start = max(news_min_date, stock_min_date)\n",
    "            overlap_end = min(news_max_date, stock_max_date)\n",
    "            \n",
    "            if overlap_start <= overlap_end:\n",
    "                print(f\"   ‚Ä¢ {symbol}: OVERLAP FOUND - {overlap_start} to {overlap_end}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {symbol}: NO OVERLAP - News: {news_min_date}-{news_max_date}, Stock: {stock_min_date}-{stock_max_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78ff0f",
   "metadata": {},
   "source": [
    "5:Comprehensive Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "089c14b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPREHENSIVE CORRELATION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Analyzing correlations for AAPL...\n",
      "   ‚Ä¢ Data points: 60\n",
      "   ‚Ä¢ Same-day correlation: 0.1254 (p=0.3399)\n",
      "   ‚Ä¢ Next-day correlation: -0.2048 (p=0.1164)\n",
      "   ‚Ä¢ Next-week correlation: 0.1008 (p=0.4596)\n",
      "\n",
      "Analyzing correlations for AMZN...\n",
      "   ‚Ä¢ Data points: 27\n",
      "   ‚Ä¢ Same-day correlation: 0.0808 (p=0.6885)\n",
      "   ‚Ä¢ Next-day correlation: -0.3197 (p=0.1040)\n",
      "   ‚Ä¢ Next-week correlation: -0.0965 (p=0.6614)\n",
      "\n",
      "Analyzing correlations for GOOG...\n",
      "   ‚Ä¢ Data points: 351\n",
      "   ‚Ä¢ Same-day correlation: 0.1793 (p=0.0007)\n",
      "   ‚Ä¢ Next-day correlation: -0.0119 (p=0.8246)\n",
      "   ‚Ä¢ Next-week correlation: 0.0654 (p=0.2245)\n",
      "\n",
      "Analyzing correlations for META...\n",
      "   ‚Ä¢ Data points: 2027\n",
      "   ‚Ä¢ Same-day correlation: 0.1511 (p=0.0000)\n",
      "   ‚Ä¢ Next-day correlation: -0.0055 (p=0.8062)\n",
      "   ‚Ä¢ Next-week correlation: -0.0078 (p=0.7267)\n",
      "\n",
      "Analyzing correlations for MSFT...\n",
      "   ‚Ä¢ Data points: 2756\n",
      "   ‚Ä¢ Same-day correlation: 0.1057 (p=0.0000)\n",
      "   ‚Ä¢ Next-day correlation: -0.0082 (p=0.6658)\n",
      "   ‚Ä¢ Next-week correlation: -0.0292 (p=0.1261)\n",
      "\n",
      "Analyzing correlations for NVDA...\n",
      "   ‚Ä¢ Data points: 1124\n",
      "   ‚Ä¢ Same-day correlation: 0.2091 (p=0.0000)\n",
      "   ‚Ä¢ Next-day correlation: -0.0210 (p=0.4827)\n",
      "   ‚Ä¢ Next-week correlation: 0.0604 (p=0.0431)\n",
      "\n",
      "============================================================\n",
      "CORRELATION ANALYSIS SUMMARY\n",
      "============================================================\n",
      "  Symbol  Data_Points  Avg_Sentiment  Avg_Return  Same_Day_Corr Same_Day_Sig  \\\n",
      "0   AAPL           60         0.0422      0.3096         0.1254            ‚úó   \n",
      "1   AMZN           27         0.0588      0.2295         0.0808            ‚úó   \n",
      "2   GOOG          351         0.0551      0.1436         0.1793            ‚úì   \n",
      "3   META         2027         0.0559      0.1172         0.1511            ‚úì   \n",
      "4   MSFT         2756         0.0572      0.0969         0.1057            ‚úì   \n",
      "5   NVDA         1124         0.0780      0.2838         0.2091            ‚úì   \n",
      "\n",
      "   Next_Day_Corr Next_Day_Sig Strength  \n",
      "0        -0.2048            ‚úó   Strong  \n",
      "1        -0.3197            ‚úó     Weak  \n",
      "2        -0.0119            ‚úó   Strong  \n",
      "3        -0.0055            ‚úó   Strong  \n",
      "4        -0.0082            ‚úó   Strong  \n",
      "5        -0.0210            ‚úó   Strong  \n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Correlation Analysis\n",
    "\n",
    "def perform_correlation_analysis(integrated_data):\n",
    "    \"\"\"\n",
    "    Perform comprehensive correlation analysis between sentiment and returns\n",
    "    \"\"\"\n",
    "    correlation_results = {}\n",
    "    \n",
    "    for symbol, data in integrated_data.items():\n",
    "        print(f\"\\nAnalyzing correlations for {symbol}...\")\n",
    "        \n",
    "        # Clean data (remove NaN values for correlation)\n",
    "        clean_data = data.dropna(subset=['sentiment_mean', 'daily_return', 'returns_tomorrow'])\n",
    "        \n",
    "        if len(clean_data) < 10:  # Minimum data points requirement\n",
    "            print(f\"   ‚ö† Insufficient data for {symbol}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate various correlations\n",
    "        correlations = {}\n",
    "        \n",
    "        # Same-day correlation\n",
    "        corr_same_day, p_value_same = pearsonr(clean_data['sentiment_mean'], clean_data['daily_return'])\n",
    "        correlations['same_day'] = {\n",
    "            'correlation': corr_same_day,\n",
    "            'p_value': p_value_same,\n",
    "            'significant': p_value_same < 0.05\n",
    "        }\n",
    "        \n",
    "        # Next-day correlation (lagged effect)\n",
    "        corr_next_day, p_value_next = pearsonr(clean_data['sentiment_mean'], clean_data['returns_tomorrow'])\n",
    "        correlations['next_day'] = {\n",
    "            'correlation': corr_next_day,\n",
    "            'p_value': p_value_next,\n",
    "            'significant': p_value_next < 0.05\n",
    "        }\n",
    "        \n",
    "        # Next-week correlation\n",
    "        clean_data_week = data.dropna(subset=['sentiment_mean', 'returns_next_week'])\n",
    "        if len(clean_data_week) >= 10:\n",
    "            corr_next_week, p_value_week = pearsonr(clean_data_week['sentiment_mean'], clean_data_week['returns_next_week'])\n",
    "            correlations['next_week'] = {\n",
    "                'correlation': corr_next_week,\n",
    "                'p_value': p_value_week,\n",
    "                'significant': p_value_week < 0.05\n",
    "            }\n",
    "        \n",
    "        # Volume-weighted sentiment correlation\n",
    "        if 'Volume' in clean_data.columns:\n",
    "            weighted_corr, weighted_p = pearsonr(\n",
    "                clean_data['sentiment_mean'], \n",
    "                clean_data['daily_return'],\n",
    "                # sample_weight=clean_data['Volume']  # Weight by trading volume\n",
    "            )\n",
    "            correlations['volume_weighted'] = {\n",
    "                'correlation': weighted_corr,\n",
    "                'p_value': weighted_p,\n",
    "                'significant': weighted_p < 0.05\n",
    "            }\n",
    "        \n",
    "        # Extreme sentiment analysis\n",
    "        high_sentiment = clean_data[clean_data['sentiment_mean'] > 0.3]\n",
    "        low_sentiment = clean_data[clean_data['sentiment_mean'] < -0.3]\n",
    "        \n",
    "        correlations['extreme_sentiment'] = {\n",
    "            'high_sentiment_days': len(high_sentiment),\n",
    "            'low_sentiment_days': len(low_sentiment),\n",
    "            'high_sentiment_avg_return': high_sentiment['daily_return'].mean() if len(high_sentiment) > 0 else 0,\n",
    "            'low_sentiment_avg_return': low_sentiment['daily_return'].mean() if len(low_sentiment) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        correlation_results[symbol] = {\n",
    "            'data_points': len(clean_data),\n",
    "            'avg_sentiment': clean_data['sentiment_mean'].mean(),\n",
    "            'avg_return': clean_data['daily_return'].mean(),\n",
    "            'sentiment_volatility': clean_data['sentiment_mean'].std(),\n",
    "            'return_volatility': clean_data['daily_return'].std(),\n",
    "            'correlations': correlations\n",
    "        }\n",
    "        \n",
    "        # Print summary for this stock\n",
    "        print(f\"   ‚Ä¢ Data points: {len(clean_data)}\")\n",
    "        print(f\"   ‚Ä¢ Same-day correlation: {corr_same_day:.4f} (p={p_value_same:.4f})\")\n",
    "        print(f\"   ‚Ä¢ Next-day correlation: {corr_next_day:.4f} (p={p_value_next:.4f})\")\n",
    "        if 'next_week' in correlations:\n",
    "            print(f\"   ‚Ä¢ Next-week correlation: {corr_next_week:.4f} (p={p_value_week:.4f})\")\n",
    "    \n",
    "    return correlation_results\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "correlation_results = perform_correlation_analysis(integrated_data)\n",
    "\n",
    "# Display overall correlation summary\n",
    "if correlation_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CORRELATION ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    summary_data = []\n",
    "    for symbol, results in correlation_results.items():\n",
    "        same_day_corr = results['correlations']['same_day']['correlation']\n",
    "        same_day_sig = results['correlations']['same_day']['significant']\n",
    "        next_day_corr = results['correlations']['next_day']['correlation']\n",
    "        next_day_sig = results['correlations']['next_day']['significant']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Symbol': symbol,\n",
    "            'Data_Points': results['data_points'],\n",
    "            'Avg_Sentiment': results['avg_sentiment'],\n",
    "            'Avg_Return': results['avg_return'],\n",
    "            'Same_Day_Corr': same_day_corr,\n",
    "            'Same_Day_Sig': '‚úì' if same_day_sig else '‚úó',\n",
    "            'Next_Day_Corr': next_day_corr,\n",
    "            'Next_Day_Sig': '‚úì' if next_day_sig else '‚úó',\n",
    "            'Strength': 'Strong' if abs(same_day_corr) > 0.1 else 'Weak'\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91aa106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week1venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
